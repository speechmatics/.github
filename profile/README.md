<h1 align="center">Hi there üëã</h1>

<p align="center">
  <a href="https://speechmatics.com">Homepage</a>
  <b> | </b><a href="https://www.speechmatics.com/about-us/company">About Us</a>
  <b> | </b><a href="https://www.speechmatics.com/about-us/careers">Careers</a>
  <b> | </b><a href="https://page.speechmatics.com/free-trial.html">Free Trial</a>
  <b> | </b><a href="https://docs.speechmatics.com">Developer Docs</a>
  <b> | </b><a href="https://github.com/speechmatics">GitHub</a>
  <b> | </b><a href="https://gitlab.com/speechmatics">GitLab</a>
  <b> | </b><a href="https://www.linkedin.com/company/speechmatics">LinkedIn</a>
</p>

<p align="center"><b><a href="https://page.speechmatics.com/portal-signup.html">Start Free Trial üöÄ</a></b></p>

<p align="center">
  <b><a href="https://www.speechmatics.com/why-speechmatics">Why Choose Speechmatics?</a></b><br>
  We have a mindset that voice technology should be an accurate representation of the world around us. This has enabled us to build the most accurate and inclusive speech-to-text engine available.
</p>

<p align="center">
  <b>Speechmatics ‚ù§ Open Source</b>
  <br>
  <a href="https://github.com/speechmatics/speechmatics-python">speechmatics-python</a>, a Python client library and CLI for Speechmatics Realtime ASR v2 API <img src="https://github.com/speechmatics/speechmatics-python/workflows/Tests/badge.svg"</img><img src="https://codecov.io/gh/speechmatics/speechmatics-python/branch/master/graph/badge.svg"></img><img src=https://img.shields.io/badge/license-MIT-yellow.svg></img>
</p>

<p align="center">
  <b>Some of our <a href="https://www.speechmatics.com/our-technology/research">research</a> üë©‚Äçüî¨</b>
  <br>
  <a href="https://arxiv.org/abs/2002.08111">Hierarchical Quantized Autoencoders</a> (code published in <a href="https://github.com/speechmatics/hqa">hqa repo</a>)
  <br>
  <i><a href="https://videos.neurips.cc/search/autoencoders/video/slideslive-38936029">NeurIPs 2020</a> - Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty. February 19, 2020.</i>
  <br>
  <a href="https://arxiv.org/abs/1910.08519">Texture Bias Of CNNs Limits Few-Shot Classification Performance</a>
  <br>
  <i>NeurIPS 2019 - Sam Ringer, Will Williams, Tom Ash, Remi Francis, David MacLeod. October 18, 2019.</i>
  <br>
  <a href="https://www.semanticscholar.org/paper/Forward-backward-retraining-of-recurrent-neural-Senior-Robinson/c25e9ebd8fe9d761f4738f7936ef114f7f6afe5d">Forward-backward retraining of recurrent neural networks</a>
  <br>
  <i>This presents the first ‚Äúend-to-end‚Äù training paper for tasks such as speech recognition. A. Senior and A.J. Robinson. Advances in Neural Information Processing Systems 8, 1996</i>
  <br>
  <a href="https://www.academia.edu/30352226/A_recurrent_error_propagation_network_speech_recognition_system">A recurrent error propagation network speech recognition system</a>
  <br>
  <i>The first application of recurrent nets to speech recognition. A.J. Robinson and F. Fallside. Computer Speech and Language, 5(3):259‚Äì274, July 1991.</i>
</p>

<p align="center">
  <b>Some of our <a href="https://medium.com/speechmatics">tech blogs</a> ‚úç</b>
  <br>
  <a href="https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd">How to Build a Streaming DataLoader with PyTorch</a>
  <br>
  <a href="https://medium.com/speechmatics/boosting-emotion-recognition-performance-in-speech-using-cpc-ce6b23a05759">How to Boost Emotion Recognition Performance in Speech Using Contrastive Predictive Coding</a>
  <br>
  <a href="https://medium.com/speechmatics/how-to-write-kubernetes-custom-controllers-in-go-8014c4a04235">How to write Kubernetes custom controllers in Go</a>
</p>
